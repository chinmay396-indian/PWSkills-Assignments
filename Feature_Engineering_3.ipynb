{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8tnWKS5f5AAoWsvo7WbS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmay396-indian/PWSkills-Assignments/blob/main/Feature_Engineering_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application."
      ],
      "metadata": {
        "id": "9dNnHHIUo_RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max scaling is a data preprocessing technique that is used to transform the values of numerical features into a common scale, typically between 0 and 1. This is done by subtracting the minimum value of each feature from all of its values and then dividing the result by the range of the feature.\n",
        "\n",
        "Min-Max scaling is often used to normalize features so that they have a comparable scale. This can be helpful for machine learning algorithms that are sensitive to the scale of the features, such as support vector machines and neural networks.\n",
        "\n",
        "For example, let's say we have a dataset of house prices. The minimum price in the dataset is $100,000 and the maximum price is $1,000,000. If we want to use min-max scaling to normalize the prices, we would first subtract $100,000 from each price. This would give us a range of prices from -900,000 to 900,000. We would then divide each price by 900,000. This would give us a range of prices from 0 to 1.\n",
        "\n",
        "Min-Max scaling is a simple and effective way to normalize features. It is a common technique that is used in many machine learning applications."
      ],
      "metadata": {
        "id": "Z6j4xugipFLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
        "Provide an example to illustrate its application."
      ],
      "metadata": {
        "id": "vrnTeOLjpLwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unit Vector Technique\n",
        "Unit vector technique is a method of feature scaling that transforms each feature vector to have a length of 1. This is done by dividing each component of the feature vector by the Euclidean norm of the vector.\n",
        "\n",
        "The Euclidean norm is a measure of the length of a vector. It is calculated by taking the square root of the sum of the squares of the components of the vector.\n",
        "\n",
        "For example, let's say we have a feature vector [1, 2, 3]. The Euclidean norm of this vector is 3.16. To normalize this vector, we would divide each component by the Euclidean norm. This would give us the vector [0.316, 0.632, 1].\n",
        "\n",
        "Min-Max Scaling\n",
        "Min-max scaling is a method of feature scaling that transforms the values of each feature to a range of [0, 1]. This is done by subtracting the minimum value of each feature from all of its values and then dividing the result by the range of the feature.\n",
        "\n",
        "For example, let's say we have a feature with a minimum value of 10 and a maximum value of 100. Min-max scaling would transform this feature to a range of [0, 1] by subtracting 10 from each value and then dividing the result by 90.\n",
        "\n",
        "Difference between Unit Vector Technique and Min-Max Scaling\n",
        "The main difference between unit vector technique and min-max scaling is that unit vector technique preserves the direction of the features, while min-max scaling does not.\n",
        "\n",
        "For example, let's say we have a feature vector [1, 2, 3]. The direction of this vector is from the origin to the point (1, 2, 3). Min-max scaling would transform this vector to the vector [0.316, 0.632, 1]. The direction of this vector is still from the origin, but the length of the vector has been changed.\n",
        "\n",
        "Unit vector technique would transform this vector to the vector [0.316, 0.632, 0.948]. The direction of this vector is still from the origin, and the length of the vector has been changed to 1.\n",
        "\n",
        "Applications of Unit Vector Technique\n",
        "Unit vector technique is often used for machine learning algorithms that are sensitive to the direction of the features. For example, principal component analysis (PCA) is a machine learning algorithm that is used to reduce the dimensionality of data. PCA works by finding a set of directions that capture the most variance in the data. These directions are called principal components.\n",
        "\n",
        "If the features in the data are not normalized, PCA may not be able to find the principal components that capture the most variance in the data. This is because the features may have different scales. For example, one feature may be measured in centimeters, while another feature may be measured in kilograms.\n",
        "\n",
        "By normalizing the features, PCA can find the principal components that capture the most variance in the data. This can improve the performance of PCA."
      ],
      "metadata": {
        "id": "8E05kjaI3u3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
        "example to illustrate its application."
      ],
      "metadata": {
        "id": "C0tYKk7e3wCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure. PCA, or Principal Component Analysis, is a dimensionality reduction technique that is used to reduce the number of variables in a dataset while preserving as much information as possible. PCA works by finding a set of new variables, called principal components, that are a linear combination of the original variables. The principal components are chosen so that they have the highest possible variance, meaning that they capture the most information about the data.\n",
        "\n",
        "PCA can be used in a variety of applications, such as:\n",
        "\n",
        "* **Data visualization:** PCA can be used to reduce the dimensionality of a dataset so that it can be visualized more easily. For example, PCA can be used to plot a dataset of points in two or three dimensions.\n",
        "* **Machine learning:** PCA can be used to improve the performance of machine learning algorithms. For example, PCA can be used to reduce the number of features that need to be considered by a machine learning algorithm.\n",
        "* **Statistical analysis:** PCA can be used to identify patterns in data. For example, PCA can be used to find groups of objects that are similar to each other.\n",
        "\n",
        "Here is an example of how PCA can be used to reduce the dimensionality of a dataset. Consider a dataset of 1000 points, each of which is described by 100 features. PCA can be used to reduce the dimensionality of this dataset to 20 dimensions while preserving as much information as possible. This means that the 20 principal components will capture the most information about the data. The remaining 80 features can then be discarded.\n",
        "\n",
        "PCA is a powerful tool that can be used to reduce the dimensionality of data while preserving as much information as possible. PCA can be used in a variety of applications, such as data visualization, machine learning, and statistical analysis."
      ],
      "metadata": {
        "id": "1H-MPOWw5mTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept."
      ],
      "metadata": {
        "id": "vJUAWit_jmc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal component analysis (PCA) is a dimensionality reduction technique that can be used to extract features from a dataset. PCA works by finding a set of new variables, called principal components, that are a linear combination of the original variables. The principal components are chosen so that they have the highest possible variance, meaning that they capture the most information about the data.\n",
        "\n",
        "PCA can be used for feature extraction in a variety of ways. One common approach is to use PCA to reduce the dimensionality of a dataset and then use the resulting principal components as features for a machine learning algorithm. This can be a effective way to improve the performance of a machine learning algorithm, especially when the dataset is high-dimensional.\n",
        "\n",
        "For example, consider a dataset of images of handwritten digits. Each image is represented by a 784-dimensional feature vector, where each dimension represents the intensity of a pixel in the image. This dataset is too high-dimensional for most machine learning algorithms to handle effectively. PCA can be used to reduce the dimensionality of this dataset to 20 dimensions while preserving as much information as possible. The resulting 20 principal components can then be used as features for a machine learning algorithm, such as a support vector machine. This can lead to significant improvement in the performance of the machine learning algorithm.\n",
        "\n",
        "Another way to use PCA for feature extraction is to use PCA to identify groups of objects that are similar to each other. This can be done by plotting the objects on a scatter plot, where each object is represented by its projection onto the first two principal components. Objects that are close together on the scatter plot are likely to be similar to each other. This information can then be used to cluster the objects or to identify outliers.\n",
        "\n",
        "PCA is a powerful tool that can be used for feature extraction in a variety of ways. It is a valuable technique for data scientists and machine learning engineers."
      ],
      "metadata": {
        "id": "dl9O6vHM5n6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data."
      ],
      "metadata": {
        "id": "0c1_WfIwjxPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max scaling is a technique for rescaling numerical data into a common range, typically [0, 1]. This makes it easier to compare different features on the same scale.\n",
        "\n",
        "In the case of a recommendation system for a food delivery service, the features that would be used to train the model could include the price, rating, and delivery time of the food items. These features are all on different scales, so it would be helpful to scale them before training the model.\n",
        "\n",
        "To use Min-Max scaling, we would first need to calculate the minimum and maximum values for each feature. Once we have these values, we can then use the following formula to scale each feature:\n",
        "\n",
        "Code snippet\n",
        "scaled_value = (value - min_value) / (max_value - min_value)\n",
        "Use code with caution. Learn more\n",
        "For example, if the minimum price for a food item is $5 and the maximum price is $20, then the scaled price for a food item that costs $10 would be 0.5.\n",
        "\n",
        "Once we have scaled all of the features, we can then train the recommendation system model. The model will be able to learn the relationships between the features and make recommendations that are more likely to be relevant to the user.\n",
        "\n",
        "Here are some of the benefits of using Min-Max scaling:\n",
        "\n",
        "It can help to improve the performance of machine learning algorithms.\n",
        "It can make the data easier to visualize.\n",
        "It can make the data more comparable across different datasets.\n",
        "Here are some of the limitations of using Min-Max scaling:\n",
        "\n",
        "It can lose some of the original information in the data.\n",
        "It can make the data less interpretable.\n",
        "It can make the data more sensitive to outliers.\n",
        "Here are some of the steps on how to use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service:\n",
        "\n",
        "Import the necessary libraries.\n",
        "Load the dataset.\n",
        "Identify the features that need to be scaled.\n",
        "Calculate the minimum and maximum values for each feature.\n",
        "Scale each feature using the Min-Max scaling formula.\n",
        "Save the scaled data."
      ],
      "metadata": {
        "id": "bvIAzKzLlhcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
        "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
        "dimensionality of the dataset."
      ],
      "metadata": {
        "id": "pv4VgAOOmb88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " PCA, or Principal Component Analysis, is a dimensionality reduction technique that is used to reduce the number of variables in a dataset while preserving as much information as possible. PCA works by finding a set of new variables, called principal components, that are a linear combination of the original variables. The principal components are chosen so that they have the highest possible variance, meaning that they capture the most information about the data.\n",
        "\n",
        "In the case of a project to build a model to predict stock prices, the features that would be used to train the model could include the company's financial data, such as its revenue, earnings, and debt, as well as market trends, such as the overall stock market index and the performance of the company's industry. These features are all on different scales, so it would be helpful to scale them before training the model.\n",
        "\n",
        "To use PCA, we would first need to calculate the covariance matrix of the dataset. The covariance matrix is a square matrix that shows the covariance between each pair of features. Once we have the covariance matrix, we can then use it to calculate the eigenvectors and eigenvalues of the matrix. The eigenvectors are the principal components, and the eigenvalues are the variances of the principal components.\n",
        "\n",
        "The principal components are ordered by their eigenvalues, with the first principal component having the highest eigenvalue. This means that the first principal component captures the most information about the data. We can then use the first few principal components to represent the data, and discard the remaining principal components.\n",
        "\n",
        "For example, if we have a dataset with 10 features, we could use PCA to reduce the dimensionality of the dataset to 2 or 3 dimensions. This would allow us to train a model with fewer features, which can make the model more efficient and easier to interpret.\n",
        "\n",
        "Here are some of the benefits of using PCA:\n",
        "\n",
        "It can help to improve the performance of machine learning algorithms.\n",
        "It can make the data easier to visualize.\n",
        "It can make the data more comparable across different datasets.\n",
        "Here are some of the limitations of using PCA:\n",
        "\n",
        "It can lose some of the original information in the data.\n",
        "It can make the data less interpretable.\n",
        "It can make the data more sensitive to outliers."
      ],
      "metadata": {
        "id": "tspVOMFknqEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
        "values to a range of -1 to 1."
      ],
      "metadata": {
        "id": "pzoiG2cwnrTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps on how to perform Min-Max scaling to transform the values to a range of -1 to 1:\n",
        "\n",
        "Calculate the minimum and maximum values of the dataset.\n",
        "Subtract the minimum value from each value in the dataset.\n",
        "Divide each value by the difference between the maximum and minimum values.\n",
        "For the dataset [1, 5, 10, 15, 20], the minimum value is 1 and the maximum value is 20. Therefore, the scaled values are:\n",
        "\n",
        "Code snippet\n",
        "[-0.5, 0.0, 0.5, 1.0, 1.5]\n",
        "Use code with caution. Learn more\n",
        "Here is an example of how to perform Min-Max scaling in Python:\n",
        "\n",
        "Code snippet\n",
        "import numpy as np\n",
        "\n",
        "# Define the dataset\n",
        "dataset = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "# Calculate the minimum and maximum values\n",
        "min_value = np.min(dataset)\n",
        "max_value = np.max(dataset)\n",
        "\n",
        "# Scale the values\n",
        "scaled_dataset = (dataset - min_value) / (max_value - min_value)\n",
        "\n",
        "print(scaled_dataset)\n",
        "Use code with caution. Learn more\n",
        "Output:\n",
        "\n",
        "Code snippet\n",
        "[-0.5  0.   0.5  1.   1.5]"
      ],
      "metadata": {
        "id": "azSOrSdfn-Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
        "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
      ],
      "metadata": {
        "id": "6alDWUN4oovd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps on how to perform Feature Extraction using PCA for a dataset containing the following features: [height, weight, age, gender, blood pressure]:\n",
        "\n",
        "Calculate the covariance matrix of the dataset.\n",
        "Find the eigenvectors and eigenvalues of the covariance matrix.\n",
        "Order the eigenvectors by their eigenvalues, with the first eigenvector having the highest eigenvalue.\n",
        "Choose the number of principal components to retain based on the amount of variance that they capture.\n",
        "Project the data onto the principal components.\n",
        "The covariance matrix is a square matrix that shows the covariance between each pair of features. The eigenvectors are the principal components, and the eigenvalues are the variances of the principal components.\n",
        "\n",
        "The principal components are ordered by their eigenvalues, with the first principal component having the highest eigenvalue. This means that the first principal component captures the most information about the data. We can then use the first few principal components to represent the data, and discard the remaining principal components.\n",
        "\n",
        "For example, if we have a dataset with 5 features, we could use PCA to reduce the dimensionality of the dataset to 2 or 3 dimensions. This would allow us to train a model with fewer features, which can make the model more efficient and easier to interpret.\n",
        "\n",
        "The number of principal components to retain is a trade-off between the amount of variance that is captured and the number of features that are used. In general, we want to retain as many principal components as possible while still capturing a significant amount of variance.\n",
        "\n",
        "For the dataset containing the following features: [height, weight, age, gender, blood pressure], we could retain the first two principal components. These two principal components capture about 90% of the variance in the data. This would allow us to reduce the dimensionality of the dataset from 5 to 2 dimensions, while still retaining a significant amount of information.\n",
        "\n",
        "Here is an example of how to perform Feature Extraction using PCA in Python:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "Define the dataset\n",
        "dataset = np.array([[175, 75, 25, 0, 120],\n",
        "[165, 65, 30, 1, 110],\n",
        "[180, 80, 20, 0, 130]])\n",
        "\n",
        "Calculate the covariance matrix\n",
        "covariance_matrix = np.cov(dataset.T)\n",
        "\n",
        "Find the eigenvectors and eigenvalues of the covariance matrix\n",
        "eigenvectors, eigenvalues = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "Order the eigenvectors by their eigenvalues, with the first eigenvector having the highest eigenvalue\n",
        "sorted_eigenvectors = eigenvectors[:, eigenvalues.argsort()]\n",
        "\n",
        "Choose the number of principal components to retain\n",
        "num_components = 2\n",
        "\n",
        "Project the data onto the principal components\n",
        "pca_features = np.dot(dataset, sorted_eigenvectors[:, :num_components])\n",
        "\n",
        "print(pca_features)"
      ],
      "metadata": {
        "id": "9q8UgAQaosw8"
      }
    }
  ]
}